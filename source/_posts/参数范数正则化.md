---
title: 参数范数正则化
date: 2017-01-12 06:49:17
tags:
categories:
- Tutorials
---
机器学习中的一个核心的问题就是解决模型的泛化能力，很多模型被故意的设计为以增大训练误差为代价来获得泛化能力，这些方法统称为正则化。很多情况下，我们通过给目标函数 $J$ 添加一个参数范数的惩罚$\Omega(\theta)$，其主要目的是用于限制模型的学习能力。正则化后的目标函数可以表示为

$$
\tilde{J}(\theta;X,y)=J(\theta;X,y)+\alpha\Omega(\theta)
$$

其中$\alpha\in[0,\infty)$是调整惩罚项惩罚力度的一个超参数。$\alpha=0$就对应着没有正则惩罚，越大的$\alpha$对应着更加严重的惩罚。当我们训练最小正则化后的目标函数时，他会同时降低原始目标的训练误差并减小参数的规模。不同的正则化会导致不同的优先解。

## $L^2$参数正则化

在$L^2$参数正则化（也被称为权重衰减 weight decay）通过向目标函数添加一个正则项$\Omega(\theta)=\frac{1}{2}\|\omega\|\_2^2$使权重更加接近原点（一般认为接近空间中的某些特定点同样拥有正则效果，这个值在接近真实值得时候会获得更好的效果，一般来说零一个是有意义的默认值）。

对$L^2$正则化的研究可以从研究目标函数的梯度入手，梯度方法是现在最流行的复杂结构的训练方法。我们先假定模型中没有偏置参数，此目标函数可以写为
$$
\tilde{J}(\omega;X,y)=\frac{\alpha}{2}\omega^T\omega+J(\omega;X,y)
$$

容易求得梯度

$$
\nabla\_\omega\tilde{J}(\omega;X,y)=\alpha\omega+\nabla\_\omega J(\omega;X,y)
$$

如果使用梯度下降更新参数的话

$$
\omega\gets\omega-\epsilon(\alpha\omega+\nabla_\omega J(\omega;X,y))
$$

为了简化分析，设$\omega^\ast$为未经过正则化过的参数向量$\omega^\ast=\arg \min_\omega J(\omega)$ 并在$\omega^\ast$的邻域做二次近似。如果目标函数是二次的（最小二乘回归或者其他以均方误差来拟合模型的情况），这个近似是无偏差的。近似的目标函数

$$
\hat{J}(\theta)=J(\omega^\ast)+\frac{1}{2}(\omega-\omega^\ast)^TH(\omega-\omega^\ast)
$$

其中$H$是$J$在$\omega^\ast$处计算的海森矩阵，因为最小化点处梯度消失，这个二次近似中理论上没有一阶项。$H$是半正定的。为了研究正则化对权重衰减的影响，我们在梯度中加入正则项，设$\hat{J}$是正则化后的最小值，使用变量$\tilde{\omega}$来表示正则化后的最小值

$$
\alpha\tilde{\omega}+H(\tilde{\omega}-\omega^\ast)=0\\(H+\alpha I)\tilde{\omega}=H\omega^\ast\\\tilde{\omega}=(H+\alpha I)^{-1}H\omega^\ast
$$

在正则化系数趋向于零时，正则化结果趋向于非正则化的结果，但是当$\alpha$增加时，因为$H$的实对称性质，我们将其对角化分解为$H=Q\Lambda Q^T$可得

$$
\tilde{\omega}=(Q\Lambda Q^T+\alpha I)^{-1}Q\Lambda Q^T\omega^\ast\\=Q(\Lambda+\alpha I)^{-1}\Lambda Q^T\omega^\ast
$$

**我们可以看到权重衰减的效果是沿着由$H$的特征向量所定义的轴对$\omega^\ast$做了缩放，具体来说，与$H$的第$i$个特征向量对其的$\omega^\ast$的分量根据$\frac{\lambda_i}{\lambda_i+\alpha}$因子做了缩放**，沿着$H$特征值较大的方向正则化影响较小，而$\lambda_i\ll\alpha$的分量将会近似被缩小到零。只有显著减小目标函数方向的参数会保存完好，不会显著增加梯度的方向将会被正则化衰减掉。**$L^2$正则化能让学习算法 ‘‘感知’’ 到具有较高方差的输入$x$，因此与输出目标的协方差较小(相对增加方差)的特征的权重将会被收缩。**

将此正则化方法应用在线性回归模型中，使得原本的平方误差函数$(X\omega-y)^T(X\omega-y)$变化为

$$
(X\omega-y)^T(X\omega-y)+\frac{\alpha}{2}\omega^T\omega
$$

方程的解从简单的伪逆解$\omega=(X^TX)^{-1}X^Ty$变为

$$
\omega=(X^TX+\alpha I)^{-1}X^Ty
$$

## $L^1$参数正则化

当对模型参数取正则化$\Omega(\theta)=\|\omega\|\_1​$时，对模型进行了$L^1​$参数正则化，与$L^2​$正则化类似，我们使用一个超参数$\alpha​$描述正则化对目标函数的贡献，此时的目标函数可以表示为

$$
\tilde{J}(\omega;X,y)=J(\omega;X,y)+\alpha\|\omega\|\_1
$$

同$L^2$的分析，此目标函数的梯度

$$
\nabla_\omega\tilde{J}(\omega;X,y)=\alpha \mathrm{sign}(\omega)+\nabla_\omega J(\omega;X,y)
$$

由此可见，$L^1$正则化并不是线性的缩放每个$\omega_i$，而是对相应的分量加上一个与$\mathrm{sign}(\omega_i)$相同符号的常数。我们一不定能得到这种正则化的直接算数解，我们可以通过泰勒级数表示目标函数是二次的线性模型，在这个设定下，梯度下降

$$
\nabla_\omega\tilde{J}(\omega)=H(\omega-\omega^\ast)
$$

$L^1$正则化在一般的情况下无法得到干净的解析表达式，我们进一步假设这个海森矩阵是对角的且每个对角元素大于零（在用于线性回归的数据已经经过类似 PCA 预处理，特征之间没有相关性得情况下，这一假设是成立的）。将$L^1$正则化的目标函数的二次近似分解为关于参数的求和

$$
\tilde{J}(\omega;X,y)=J(\omega;X,y)+\sum_i\Big[\frac{1}{2}H_{ii}(\omega_i-\omega_i^\ast)^2+\alpha|\omega_i|\Big]
$$

对这个近似的目标函数，我们可以得到每一维的解析解

$$
\omega_i=\mathrm{sign}(\omega_i^\ast)\max\Big\{|\omega_i^\ast|-\frac{\alpha}{H_{ii}},0\Big\}
$$

现考虑所有$i$且$\omega_i^\ast>0$的情况：

1. $\omega\_i^\ast\leq\frac{\alpha}{H\_{ii}}$ 的情况，正则化后给出的最优值是$\omega_i=0$。这是因为在方向$i$上$J(\omega;X,y)$对于 $\tilde{J}(\omega;X,y)$的贡献受到压制，$L^1$正这话将这个参数推向零
2. $\omega\_i^\ast>\frac{\alpha}{H\_{ii}}$ 时，正则化仅仅在那个方向上移动$\frac{\alpha}{H\_{ii}}$ 的距离

$\omega\_i^\ast$的情况与之类似，但是正则化使$\omega_i^\ast$更接近0或者为0.

一般来说$L^1$正则化倾向产生更**稀疏**的解，$L^2$正则化虽然对一些输入进行了缩放，但是并不会使参数稀疏，$L^1$可以通过足够大的$\alpha$实现稀疏。也正是因为$L^1$的稀疏性质，这种正则化被广泛地用于特征选择。就如著名的 LASSO (Least Absolute Shrinkage and Selection Operator)将$L^1$正则化与线性模型结合，并使用最小二乘使部分子集的权重为零，表明某些特征可以被安全的忽略。

## MAP 贝叶斯推断正则化

许多的正则化策略可以解释为 MAP 贝叶斯推断。先看原始的线性回归模型，我们的假设为$h_\omega(x)=\omega^Tx+\epsilon$，其中$\epsilon$是一个呈高斯分布的误差量$p(\epsilon)=N(0,\sigma^2)$我们可以得出对数似然函数

$$
l(\omega)=\log L(\omega)=\prod_{i=1}^mp(y^{(i)}|X;\omega)\\=\sum_{i=1}^{m}\log\frac{1}{\sqrt{2\pi}\sigma}\exp\Big(-\frac{(y^{(i)}-\omega^Tx^{(i)})^2)}{2\sigma^2}\Big)\\= m\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\omega^Tx^{(i)})^2
$$

这是一般的最小二乘优化形式。这时我们对参数没有加入任何的先验分布，在模型参数很多的时候很容易发生过拟合，通过对$\omega$引入不同的先验分布，我们可以同样推导出我们熟悉的正则化形式。

1. $L^2$正则化相当于权重是高斯先验的 MAP 贝叶斯推断。

  当我们对参数引入协方差为$\alpha$的零均值高斯先验 $p(\omega_j)=\frac{1}{\sqrt{2\pi\alpha}}\exp(-(\omega^{(j)})^2/2\alpha)$，其最大后验估计

  $$
  \begin{equation}
  \begin{split}
  L(\omega)&=p(y|X;\omega)p(\omega)\\
  &=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\Bigg(- \frac{(y^{(i)}-\omega^Tx^{(i)})^2)}{2\sigma^2}\Bigg)\prod_{j=1}^m
  \frac{1}{\sqrt{2\pi\alpha}}\exp \Bigg(-\frac{(\omega^{(j)})^2}{2\alpha}\Bigg)\\
  &=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\Bigg(- \frac{(y^{(i)}-\omega^Tx^{(i)})^2)}{2\sigma^2}\Bigg)\frac{1}{\sqrt{2\pi\alpha}}\exp \Bigg(-\frac{\omega^T\omega}{2\alpha}\Bigg)
  \end{split}
  \end{equation}
  $$

  取其对数

  $$
  \begin{equation}\begin{split}
  l(\omega)&=\log l(\omega)\\
  &=m\log\frac{1}{\sqrt{2\pi}\sigma}+n\log\frac{1}{\sqrt{2\pi\alpha}}-\frac{1}{2\sigma^2}\sum_{i=1}^m(y^{(i)}-\omega^Tx^{(i)})^2-\frac{1}{2\alpha}\omega^T\omega
  \end{split}\end{equation}
  $$

  最大似然后 $\omega=\mathrm{argmin}\_\omega(\frac{1}{n}\|y-\omega^TX\|\_2+\alpha\|\omega\|\_2)$ ，提取其目标函数，与$L^2$正则化形式类似。

2. $L^1$正则化相当于是权重为拉普拉斯先验的 MAP 贝叶斯推断
